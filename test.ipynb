{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils.encode_process_decode import LazyMLP\n",
    "from model_utils.HyperEl import Model\n",
    "import torch\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (_output_normalizer): Normalizer()\n",
      "  (_mesh_edge_normalizer): Normalizer()\n",
      "  (_world_edge_normalizer): Normalizer()\n",
      "  (learned_model): EncodeProcessDecode(\n",
      "    (encoder): Encoder(\n",
      "      (node_model): Sequential(\n",
      "        (0): LazyMLP(\n",
      "          (layers): Sequential(\n",
      "            (linear_0): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "            (relu_0): Sigmoid()\n",
      "            (linear_1): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "            (relu_1): Sigmoid()\n",
      "            (linear_2): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (mesh_edge_model): Sequential(\n",
      "        (0): LazyMLP(\n",
      "          (layers): Sequential(\n",
      "            (linear_0): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "            (relu_0): Sigmoid()\n",
      "            (linear_1): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "            (relu_1): Sigmoid()\n",
      "            (linear_2): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (world_edge_model): Sequential(\n",
      "        (0): LazyMLP(\n",
      "          (layers): Sequential(\n",
      "            (linear_0): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "            (relu_0): Sigmoid()\n",
      "            (linear_1): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "            (relu_1): Sigmoid()\n",
      "            (linear_2): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (processor): Processor(\n",
      "      (graphnet_blocks): ModuleList(\n",
      "        (0-14): 15 x GraphNetBlock(\n",
      "          (mesh_edge_model): Sequential(\n",
      "            (0): LazyMLP(\n",
      "              (layers): Sequential(\n",
      "                (linear_0): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "                (relu_0): Sigmoid()\n",
      "                (linear_1): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "                (relu_1): Sigmoid()\n",
      "                (linear_2): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (world_edge_model): Sequential(\n",
      "            (0): LazyMLP(\n",
      "              (layers): Sequential(\n",
      "                (linear_0): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "                (relu_0): Sigmoid()\n",
      "                (linear_1): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "                (relu_1): Sigmoid()\n",
      "                (linear_2): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (node_model): Sequential(\n",
      "            (0): LazyMLP(\n",
      "              (layers): Sequential(\n",
      "                (linear_0): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "                (relu_0): Sigmoid()\n",
      "                (linear_1): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "                (relu_1): Sigmoid()\n",
      "                (linear_2): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (linear_layer): LazyLinear(in_features=0, out_features=1, bias=True)\n",
      "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (model): LazyMLP(\n",
      "        (layers): Sequential(\n",
      "          (linear_0): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "          (relu_0): Sigmoid()\n",
      "          (linear_1): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "          (relu_1): Sigmoid()\n",
      "          (linear_2): LazyLinear(in_features=0, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\deeplearning\\envs\\gnn\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "n = Model(2)\n",
    "\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "from model_utils import normalization\n",
    "from model_utils import Cloth\n",
    "\n",
    "n = Cloth.Model(3)\n",
    "# n = normalization.Normalizer(3,'n')\n",
    "n.to('cpu')\n",
    "print(n._mesh_edge_normalizer._acc_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b']\n"
     ]
    }
   ],
   "source": [
    "d = {\n",
    "    \"a\":1,\n",
    "    \"b\":2\n",
    "}\n",
    "print(list(d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups: ['ex0', 'ex1', 'ex10', 'ex100', 'ex101', 'ex102', 'ex103', 'ex104', 'ex105', 'ex106', 'ex107', 'ex108', 'ex109', 'ex11', 'ex110', 'ex111', 'ex112', 'ex113', 'ex114', 'ex115', 'ex116', 'ex117', 'ex118', 'ex119', 'ex12', 'ex120', 'ex121', 'ex122', 'ex123', 'ex124', 'ex125', 'ex126', 'ex127', 'ex128', 'ex129', 'ex13', 'ex130', 'ex131', 'ex132', 'ex133', 'ex134', 'ex135', 'ex136', 'ex137', 'ex138', 'ex139', 'ex14', 'ex140', 'ex141', 'ex142', 'ex143', 'ex144', 'ex145', 'ex146', 'ex147', 'ex148', 'ex149', 'ex15', 'ex150', 'ex151', 'ex152', 'ex153', 'ex154', 'ex155', 'ex156', 'ex157', 'ex158', 'ex159', 'ex16', 'ex160', 'ex161', 'ex162', 'ex163', 'ex164', 'ex165', 'ex166', 'ex167', 'ex168', 'ex169', 'ex17', 'ex170', 'ex171', 'ex172', 'ex173', 'ex174', 'ex175', 'ex176', 'ex177', 'ex178', 'ex179', 'ex18', 'ex180', 'ex181', 'ex182', 'ex183', 'ex184', 'ex185', 'ex186', 'ex187', 'ex188', 'ex189', 'ex19', 'ex190', 'ex191', 'ex192', 'ex193', 'ex194', 'ex195', 'ex196', 'ex197', 'ex198', 'ex199', 'ex2', 'ex20', 'ex200', 'ex201', 'ex202', 'ex203', 'ex204', 'ex205', 'ex206', 'ex207', 'ex208', 'ex209', 'ex21', 'ex210', 'ex211', 'ex212', 'ex213', 'ex214', 'ex215', 'ex216', 'ex217', 'ex218', 'ex219', 'ex22', 'ex220', 'ex221', 'ex222', 'ex223', 'ex224', 'ex225', 'ex226', 'ex227', 'ex228', 'ex229', 'ex23', 'ex230', 'ex231', 'ex232', 'ex233', 'ex234', 'ex235', 'ex236', 'ex237', 'ex238', 'ex239', 'ex24', 'ex240', 'ex241', 'ex242', 'ex243', 'ex244', 'ex245', 'ex246', 'ex247', 'ex248', 'ex249', 'ex25', 'ex250', 'ex251', 'ex252', 'ex253', 'ex254', 'ex255', 'ex256', 'ex257', 'ex258', 'ex259', 'ex26', 'ex260', 'ex261', 'ex262', 'ex263', 'ex264', 'ex265', 'ex266', 'ex267', 'ex268', 'ex269', 'ex27', 'ex270', 'ex271', 'ex272', 'ex273', 'ex274', 'ex275', 'ex276', 'ex277', 'ex278', 'ex279', 'ex28', 'ex280', 'ex281', 'ex282', 'ex283', 'ex284', 'ex285', 'ex286', 'ex287', 'ex288', 'ex289', 'ex29', 'ex290', 'ex291', 'ex292', 'ex293', 'ex294', 'ex295', 'ex296', 'ex297', 'ex298', 'ex299', 'ex3', 'ex30', 'ex300', 'ex301', 'ex302', 'ex303', 'ex304', 'ex305', 'ex306', 'ex307', 'ex308', 'ex309', 'ex31', 'ex310', 'ex311', 'ex312', 'ex313', 'ex314', 'ex315', 'ex316', 'ex317', 'ex318', 'ex319', 'ex32', 'ex320', 'ex321', 'ex322', 'ex323', 'ex324', 'ex325', 'ex326', 'ex327', 'ex328', 'ex329', 'ex33', 'ex330', 'ex331', 'ex332', 'ex333', 'ex334', 'ex335', 'ex336', 'ex337', 'ex338', 'ex339', 'ex34', 'ex340', 'ex341', 'ex342', 'ex343', 'ex344', 'ex345', 'ex346', 'ex347', 'ex348', 'ex349', 'ex35', 'ex350', 'ex351', 'ex352', 'ex353', 'ex354', 'ex355', 'ex356', 'ex357', 'ex358', 'ex359', 'ex36', 'ex360', 'ex361', 'ex362', 'ex363', 'ex364', 'ex365', 'ex366', 'ex367', 'ex368', 'ex369', 'ex37', 'ex370', 'ex371', 'ex372', 'ex373', 'ex374', 'ex375', 'ex376', 'ex377', 'ex378', 'ex379', 'ex38', 'ex380', 'ex381', 'ex382', 'ex383', 'ex384', 'ex385', 'ex386', 'ex387', 'ex388', 'ex389', 'ex39', 'ex390', 'ex391', 'ex392', 'ex393', 'ex394', 'ex395', 'ex396', 'ex397', 'ex398', 'ex399', 'ex4', 'ex40', 'ex400', 'ex401', 'ex402', 'ex403', 'ex404', 'ex405', 'ex406', 'ex407', 'ex408', 'ex409', 'ex41', 'ex410', 'ex411', 'ex412', 'ex413', 'ex414', 'ex415', 'ex416', 'ex417', 'ex418', 'ex419', 'ex42', 'ex420', 'ex421', 'ex422', 'ex423', 'ex424', 'ex425', 'ex426', 'ex427', 'ex428', 'ex429', 'ex43', 'ex430', 'ex431', 'ex432', 'ex433', 'ex434', 'ex435', 'ex436', 'ex437', 'ex438', 'ex439', 'ex44', 'ex440', 'ex441', 'ex442', 'ex443', 'ex444', 'ex445', 'ex446', 'ex447', 'ex448', 'ex449', 'ex45', 'ex450', 'ex451', 'ex452', 'ex453', 'ex454', 'ex455', 'ex456', 'ex457', 'ex458', 'ex459', 'ex46', 'ex460', 'ex461', 'ex462', 'ex463', 'ex464', 'ex465', 'ex466', 'ex467', 'ex468', 'ex469', 'ex47', 'ex470', 'ex471', 'ex472', 'ex473', 'ex474', 'ex475', 'ex476', 'ex477', 'ex478', 'ex479', 'ex48', 'ex480', 'ex481', 'ex482', 'ex483', 'ex484', 'ex485', 'ex486', 'ex487', 'ex488', 'ex489', 'ex49', 'ex490', 'ex491', 'ex492', 'ex493', 'ex494', 'ex495', 'ex496', 'ex497', 'ex498', 'ex499', 'ex5', 'ex50', 'ex500', 'ex501', 'ex502', 'ex503', 'ex504', 'ex505', 'ex506', 'ex507', 'ex508', 'ex509', 'ex51', 'ex510', 'ex511', 'ex512', 'ex513', 'ex514', 'ex515', 'ex516', 'ex517', 'ex518', 'ex519', 'ex52', 'ex520', 'ex521', 'ex522', 'ex523', 'ex524', 'ex525', 'ex526', 'ex527', 'ex528', 'ex529', 'ex53', 'ex530', 'ex531', 'ex532', 'ex533', 'ex534', 'ex535', 'ex536', 'ex537', 'ex538', 'ex539', 'ex54', 'ex540', 'ex541', 'ex542', 'ex543', 'ex544', 'ex545', 'ex546', 'ex547', 'ex548', 'ex549', 'ex55', 'ex550', 'ex551', 'ex552', 'ex553', 'ex554', 'ex555', 'ex556', 'ex557', 'ex558', 'ex559', 'ex56', 'ex560', 'ex561', 'ex562', 'ex563', 'ex564', 'ex565', 'ex566', 'ex567', 'ex568', 'ex569', 'ex57', 'ex570', 'ex571', 'ex572', 'ex573', 'ex574', 'ex575', 'ex576', 'ex577', 'ex578', 'ex579', 'ex58', 'ex580', 'ex581', 'ex582', 'ex583', 'ex584', 'ex585', 'ex586', 'ex587', 'ex588', 'ex589', 'ex59', 'ex590', 'ex591', 'ex592', 'ex593', 'ex594', 'ex595', 'ex596', 'ex597', 'ex598', 'ex599', 'ex6', 'ex60', 'ex600', 'ex601', 'ex602', 'ex603', 'ex604', 'ex605', 'ex606', 'ex607', 'ex608', 'ex609', 'ex61', 'ex610', 'ex611', 'ex612', 'ex613', 'ex614', 'ex615', 'ex616', 'ex617', 'ex618', 'ex619', 'ex62', 'ex620', 'ex621', 'ex622', 'ex623', 'ex624', 'ex625', 'ex626', 'ex627', 'ex628', 'ex629', 'ex63', 'ex630', 'ex631', 'ex632', 'ex633', 'ex634', 'ex635', 'ex636', 'ex637', 'ex638', 'ex639', 'ex64', 'ex640', 'ex641', 'ex642', 'ex643', 'ex644', 'ex645', 'ex646', 'ex647', 'ex648', 'ex649', 'ex65', 'ex650', 'ex651', 'ex652', 'ex653', 'ex654', 'ex655', 'ex656', 'ex657', 'ex658', 'ex659', 'ex66', 'ex660', 'ex661', 'ex662', 'ex663', 'ex664', 'ex665', 'ex666', 'ex667', 'ex668', 'ex669', 'ex67', 'ex670', 'ex671', 'ex672', 'ex673', 'ex674', 'ex675', 'ex676', 'ex677', 'ex678', 'ex679', 'ex68', 'ex680', 'ex681', 'ex682', 'ex683', 'ex684', 'ex685', 'ex686', 'ex687', 'ex688', 'ex689', 'ex69', 'ex690', 'ex691', 'ex692', 'ex693', 'ex694', 'ex695', 'ex696', 'ex697', 'ex698', 'ex699', 'ex7', 'ex70', 'ex700', 'ex701', 'ex702', 'ex703', 'ex704', 'ex705', 'ex706', 'ex707', 'ex708', 'ex709', 'ex71', 'ex710', 'ex711', 'ex712', 'ex713', 'ex714', 'ex715', 'ex716', 'ex717', 'ex718', 'ex719', 'ex72', 'ex720', 'ex721', 'ex722', 'ex723', 'ex724', 'ex725', 'ex726', 'ex727', 'ex728', 'ex729', 'ex73', 'ex730', 'ex731', 'ex732', 'ex733', 'ex734', 'ex735', 'ex736', 'ex737', 'ex738', 'ex739', 'ex74', 'ex740', 'ex741', 'ex742', 'ex743', 'ex744', 'ex745', 'ex746', 'ex747', 'ex748', 'ex749', 'ex75', 'ex750', 'ex751', 'ex752', 'ex753', 'ex754', 'ex755', 'ex756', 'ex757', 'ex758', 'ex759', 'ex76', 'ex760', 'ex761', 'ex762', 'ex763', 'ex764', 'ex765', 'ex766', 'ex767', 'ex768', 'ex769', 'ex77', 'ex770', 'ex771', 'ex772', 'ex773', 'ex774', 'ex775', 'ex776', 'ex777', 'ex778', 'ex779', 'ex78', 'ex780', 'ex781', 'ex782', 'ex783', 'ex784', 'ex785', 'ex786', 'ex787', 'ex788', 'ex789', 'ex79', 'ex790', 'ex791', 'ex792', 'ex793', 'ex794', 'ex795', 'ex796', 'ex797', 'ex798', 'ex799', 'ex8', 'ex80', 'ex800', 'ex801', 'ex802', 'ex803', 'ex804', 'ex805', 'ex806', 'ex807', 'ex808', 'ex809', 'ex81', 'ex810', 'ex811', 'ex812', 'ex813', 'ex814', 'ex815', 'ex816', 'ex817', 'ex818', 'ex819', 'ex82', 'ex820', 'ex821', 'ex822', 'ex823', 'ex824', 'ex825', 'ex826', 'ex827', 'ex828', 'ex829', 'ex83', 'ex830', 'ex831', 'ex832', 'ex833', 'ex834', 'ex835', 'ex836', 'ex837', 'ex838', 'ex839', 'ex84', 'ex840', 'ex841', 'ex842', 'ex843', 'ex844', 'ex845', 'ex846', 'ex847', 'ex848', 'ex849', 'ex85', 'ex850', 'ex851', 'ex852', 'ex853', 'ex854', 'ex855', 'ex856', 'ex857', 'ex858', 'ex859', 'ex86', 'ex860', 'ex861', 'ex862', 'ex863', 'ex864', 'ex865', 'ex866', 'ex867', 'ex868', 'ex869', 'ex87', 'ex870', 'ex871', 'ex872', 'ex873', 'ex874', 'ex875', 'ex876', 'ex877', 'ex878', 'ex879', 'ex88', 'ex880', 'ex881', 'ex882', 'ex883', 'ex884', 'ex885', 'ex886', 'ex887', 'ex888', 'ex889', 'ex89', 'ex890', 'ex891', 'ex892', 'ex893', 'ex894', 'ex895', 'ex896', 'ex897', 'ex898', 'ex899', 'ex9', 'ex90', 'ex900', 'ex901', 'ex902', 'ex903', 'ex904', 'ex905', 'ex906', 'ex907', 'ex908', 'ex909', 'ex91', 'ex910', 'ex911', 'ex912', 'ex913', 'ex914', 'ex915', 'ex916', 'ex917', 'ex918', 'ex919', 'ex92', 'ex920', 'ex921', 'ex922', 'ex923', 'ex924', 'ex925', 'ex926', 'ex927', 'ex928', 'ex929', 'ex93', 'ex930', 'ex931', 'ex932', 'ex933', 'ex934', 'ex935', 'ex936', 'ex937', 'ex938', 'ex939', 'ex94', 'ex940', 'ex941', 'ex942', 'ex943', 'ex944', 'ex945', 'ex946', 'ex947', 'ex948', 'ex949', 'ex95', 'ex950', 'ex951', 'ex952', 'ex953', 'ex954', 'ex955', 'ex956', 'ex957', 'ex958', 'ex959', 'ex96', 'ex960', 'ex961', 'ex962', 'ex963', 'ex964', 'ex965', 'ex966', 'ex967', 'ex968', 'ex969', 'ex97', 'ex970', 'ex971', 'ex972', 'ex973', 'ex974', 'ex975', 'ex976', 'ex977', 'ex978', 'ex979', 'ex98', 'ex980', 'ex981', 'ex982', 'ex983', 'ex984', 'ex985', 'ex986', 'ex987', 'ex988', 'ex989', 'ex99', 'ex990', 'ex991', 'ex992', 'ex993', 'ex994', 'ex995', 'ex996', 'ex997', 'ex998', 'ex999']\n",
      "Datasets: <class 'h5py._hl.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# 以只读模式打开HDF5文件\n",
    "file = h5py.File('D:/project_summary/Graduation Project/tmp/datasets_hdf5/flag_simple/train/dataset.h5', 'r')\n",
    "\n",
    "\n",
    "# 获取文件中的所有组\n",
    "groups = list(file.keys())\n",
    "print('Groups:', groups)\n",
    "dataset = file['ex0']\n",
    "# 获取文件中的所有数据集\n",
    "print('Datasets:', type(dataset['world_pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.Tensor([1,2])\n",
    "b = torch.Tensor([2,3])\n",
    "print(torch.concat((a,b),dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "print(a[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个全False的矩阵\n",
    "matrix_shape = (5, 5)  # 示例形状\n",
    "matrix = torch.zeros(matrix_shape, dtype=torch.bool)\n",
    "\n",
    "# 指定的行和列范围\n",
    "i, j = 1, 3  # 行范围\n",
    "n, m = 0, 2  # 列范围\n",
    "\n",
    "\n",
    "\n",
    "# 设置指定列范围内的所有元素为 True\n",
    "matrix[i:j+1, n:m+1] = True\n",
    "\n",
    "# 交叉部分会被设置为 True 两次，但这不会影响结果\n",
    "new_matrix = torch.zeros(matrix_shape, dtype=torch.bool)\n",
    "print(matrix & new_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "1 2\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "def test(a,b):\n",
    "    a = 1\n",
    "    print(a,b)\n",
    "\n",
    "a = 2\n",
    "b= 2\n",
    "print(a,b)\n",
    "test(a,b)\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0381, -0.0177],\n",
      "        [-0.0497, -0.0789]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.Tensor([[2,3],[4,5]])\n",
    "zero_size = torch.zeros(a.size(), dtype=torch.float32)\n",
    "noise = torch.normal(zero_size, std=0.1)\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 6])\n",
      "tensor([[2],\n",
      "        [2],\n",
      "        [2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [2],\n",
      "        [5],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个示例 tensor\n",
    "tensor = torch.tensor([[1], [2], [3], [4], [2], [5], [2]])\n",
    "\n",
    "# 选出等于特定值（例如 2）的元素对应的序号\n",
    "value = 2\n",
    "indices = torch.nonzero(tensor.squeeze() == value).squeeze()\n",
    "\n",
    "print(indices)\n",
    "print(tensor[indices])\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sparse Tensor:\n",
      "tensor(indices=tensor([[0, 0, 1, 1, 2, 2, 2],\n",
      "                       [1, 1, 2, 2, 0, 0, 0]]),\n",
      "       values=tensor([1., 2., 3., 4., 5., 6., 7.]),\n",
      "       size=(3, 3), nnz=7, layout=torch.sparse_coo)\n",
      "\n",
      "Coalesced Indices:\n",
      "tensor([[0, 1, 2],\n",
      "        [1, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 创建一个稀疏张量\n",
    "indices = torch.tensor([[0, 0, 1, 1, 2, 2, 2],\n",
    "[1, 1, 2, 2, 0, 0, 0]])\n",
    "values = torch.tensor([1, 2, 3, 4, 5, 6, 7], dtype=torch.float32)\n",
    "size = torch.Size([3, 3])\n",
    "\n",
    "world_connection_matrix = torch.sparse_coo_tensor(indices, values, size)\n",
    "\n",
    "# 打印原始稀疏张量\n",
    "print(\"Original Sparse Tensor:\")\n",
    "print(world_connection_matrix)\n",
    "\n",
    "# 使用 coalesce() 合并重复索引\n",
    "coalesced_matrix = world_connection_matrix.coalesce()\n",
    "\n",
    "# 获取合并后的索引\n",
    "coalesced_indices = coalesced_matrix.indices()\n",
    "\n",
    "print(\"\\nCoalesced Indices:\")\n",
    "print(coalesced_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0]),)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "indices = torch.tensor([[0, 0, 1, 1, 2, 2, 2],\n",
    "[0, 1, 2, 2, 0, 0, 0]])\n",
    "a = (indices[0] == indices[1]).nonzero(as_tuple=True)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
